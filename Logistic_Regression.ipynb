{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ASSIGNMENT QUESTIONS"
      ],
      "metadata": {
        "id": "jlcbmK6sHhzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 : What is Simple Linear Regression (SLR)? Explain its purpose.**\n",
        "\n",
        "Answer:\n",
        "- Simple Linear Regression (SLR) is a statistical method that models the relationship between a single independent variable and a single dependent variable using a straight line.\n",
        "- Purpose of Simple Linear Regression:\n",
        "  - To model relationships: SLR finds the straight line that best fits the data points for two continuous variables, one independent (predictor) and one dependent (outcome).\n",
        "  - To understand influence: It helps quantify the strength and direction of the relationship. The slope of the line indicates how much the dependent variable is expected to change for a one-unit increase in the independent variable.\n",
        "  - To make predictions: Once the best-fit line is established, you can use its equation (y=mx+b) to predict the value of the dependent variable for a new value of the independent variable.\n",
        "  - To serve as a foundation: Although simple, SLR is a foundational concept for more complex models like multiple linear regression, which involves more than one independent variable\n",
        "\n",
        "**Question 2: What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "Answer:\n",
        "- Key assumptions of Simple Linear Regression:\n",
        "  - Linearity: There is a linear relationship between the independent (predictor) variable and the dependent (outcome) variable.\n",
        "  - Independence of Errors: The residuals (errors) are independent of each other and are not correlated.\n",
        "  - Homoscedasticity (Constant Variance): The variance of the residuals is constant across all levels of the independent variable.\n",
        "  - Normality of Residuals: The residuals are normally distributed, typically with a mean of zero.\n",
        "  - Zero Mean of Errors: The expected value (mean) of the errors is zero.\n",
        "  - No Perfect Multicollinearity: This is primarily an assumption for multiple linear regression, but for simple linear regression, it means the independent variable is not perfectly correlated with itself. Since there's only one predictor in simple linear regression, this is a trivial condition.\n",
        "\n",
        "**Question 3: Write the mathematical equation for a simple linear regression model and explain each term.**\n",
        "\n",
        "Answer:\n",
        "- The mathematical equation for a simple linear regression model is ùëå = ùõΩ0+ùõΩ1ùëã+ùúñ In this equation, Y is the dependent variable, X is the independent variable, ùõΩ0 is the y-intercept, ùõΩ1 is the slope, and ùúñ is the random error term.\n",
        "- The equation and its terms:\n",
        "  - ùëå: The dependent variable. This is the variable you are trying to predict.\n",
        "  - ùëã: The independent variable. This is the variable used to predict the dependent variable.\n",
        "  - ùõΩ0: The y-intercept. This is the value of Y when X is equal to 0.\n",
        "  - ùõΩ1: The slope of the line. This represents the change in the dependent variable Y for a one-unit change in the independent variable X\n",
        "  - ùúñ: The error term. This accounts for the variability in Y that cannot be explained by the linear relationship with X. It represents the difference between the actual value of Y and the predicted value from the line.\n",
        "- The simplified equation:\n",
        "  Often, the error term is omitted to represent the predicted value of Y, resulting in the simplified equation: ùëå = ùõΩ0+ùõΩ1ùëã\n",
        "\n",
        "Question 4: Provide a real-world example where simple linear regression can be\n",
        "applied. **bold text**\n",
        "\n",
        "Answer:\n",
        "- A real-world example of simple linear regression is predicting a company's sales based on its advertising spending. In this case, the independent variable is the amount of money spent on advertising, and the dependent variable is the total sales. By analyzing past data, a linear regression model can identify the relationship between these two variables and then be used to predict future sales by estimating the impact of different advertising budgets.¬†   \n",
        "- Independent Variable (X): Advertising spend\n",
        "- Dependent Variable (Y): Total sales\n",
        "- Goal: To find a linear equation (Y=a+bX) that describes how sales change as advertising spend changes.\n",
        "- Application: A company could use the resulting model to forecast how much they might sell if they invest a certain amount in advertising, helping them make better decisions about their marketing budget.\n",
        "\n",
        "**Question 5: What is the method of least squares in linear regression?**\n",
        "\n",
        "Answer:\n",
        "- The method of least squares is a statistical technique used in linear regression to find the best-fit line for a set of data points by minimizing the sum of the squares of the vertical distances (residuals) between the data points and the line. This is achieved by finding the line that creates the smallest possible sum of squared errors, which gives the \"least squares\" name to the method.\n",
        "- How it works:\n",
        "  - Data points: You start with a set of data points, often plotted as a scatter plot, where each point is an (x,y) coordinate.\n",
        "  - Initial line: A line is drawn through the data points. This initial line will not perfectly fit the data.\n",
        "  - Residuals: The vertical distance between each data point and the line is calculated. This distance is called the residual, or error.\n",
        "  - Squared residuals: Each residual is squared. This is done to ensure that negative and positive errors don't cancel each other out and to penalize larger errors more heavily.\n",
        "  - Sum of squared residuals: All the squared residuals are added together to get the sum of squared errors.\n",
        "  - Minimization: The method of least squares finds the specific slope (m) and y-intercept (c) for the line (y=mx+c) that results in the smallest possible value for this sum of squared residuals.\n",
        "- Purpose and application:\n",
        "  - The resulting line is called the \"line of best fit\" and provides a quantitative model for the relationship between two variables.\n",
        "  - It is widely used in statistics, machine learning, and engineering to make predictions or understand trends in data.  \n",
        "\n",
        "**Question 6: What is Logistic Regression? How does it differ from Linear Regression?**\n",
        "\n",
        "Answer:  \n",
        "- Logistic regression predicts categorical outcomes (like yes/no) using a probability output between 0 and 1, while linear regression predicts continuous outcomes (like price or temperature) with a value that can range from negative to positive infinity. The key difference lies in their goal: logistic regression is for classification, and linear regression is for regression problems.\n",
        "- Linear Regression:\n",
        "  - Linear Regression is a supervised regression model.\n",
        "  - Equation of linear regression:(a0+a1x1+a2x2+‚ãØ+aixi)(a0+a1x1+a2x2+‚ãØ+aixi)\n",
        "    Here,\n",
        "    y = response variable\n",
        "    xi = ith predictor variable\n",
        "    ai= average effect on y as xi increases by 1\n",
        "  - In Linear Regression, we predict the value by an integer number.\n",
        "  - Here no activation function is used.\n",
        "  - Here no threshold value is needed.\n",
        "  - Here we calculate Root Mean Square Error(RMSE) to predict the next weight value.\n",
        "  - Here dependent variable should be numeric and the response variable is continuous to value.\n",
        "  - It is based on the least square estimation.\n",
        "  - Here when we plot the training datasets, a straight line can be drawn that touches maximum plots.\n",
        "  - Linear regression is used to estimate the dependent variable in case of a change in independent variables. For example, predict the price of houses.\n",
        "  - Linear regression assumes the normal or gaussian distribution of the dependent variable.\n",
        "  - Applications of linear regression:\n",
        "    - Financial risk assessment\n",
        "    - Business insights\n",
        "    - Market analysis  \n",
        "- Logistic Regression:\n",
        "  - Logistic Regression is a supervised classification model.\n",
        "  - Equation of logistic regression:y(x)=e(a0+a1x1+a2x2+‚ãØ+aixi)/1+e(a0+a1x1+a2x2+‚ãØ+aixi)\n",
        "    Here,\n",
        "    y = response variable\n",
        "    xi = ith predictor variable\n",
        "    ai = average effect on y as xi increases by 1\n",
        "  - In Logistic Regression, we predict the value by 1 or 0.\n",
        "  - Here activation function is used to convert a linear regression equation to the logistic regression equation\n",
        "  - Here a threshold value is added.\n",
        "  - Here we use precision to predict the next weight value.\n",
        "  - Here the dependent variable consists of only two categories. Logistic regression estimates the odds outcome of the dependent variable given a set of quantitative or categorical independent variables.\n",
        "  - It is based on maximum likelihood estimation.\n",
        "  - Any change in the coefficient leads to a change in both the direction and the steepness of the logistic function. It means positive slopes result in an S-shaped curve and negative slopes result in a Z-shaped curve.\n",
        "  - Whereas logistic regression is used to calculate the probability of an event. For example, classify if tissue is benign or malignant.\n",
        "  - Logistic regression assumes the binomial distribution of the dependent variable.\n",
        "  - Applications of logistic regression:\n",
        "    - Medicine\n",
        "    - Credit scoring\n",
        "    - Hotel Booking\n",
        "    - Gaming\n",
        "    - Text editing  \n",
        "\n",
        "**Question 7: Name and briefly describe three common evaluation metrics for regression models.**\n",
        "\n",
        "Answer:\n",
        "- Three common regression metrics are R-squared (\\(R^{2}\\)), which measures the proportion of variance in the dependent variable explained by the model; Mean Absolute Error (MAE), which calculates the average absolute difference between predicted and actual values; and Root Mean Squared Error (RMSE), which is the square root of the mean of the squared errors and penalizes larger errors more heavily than MAE\n",
        "- 1. R-squared (R^{2}):\n",
        "  - Description: Also known as the coefficient of determination, (R^{2}) indicates the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
        "  - Interpretation: An (R^{2}) of (1) means the model perfectly predicts the target variable, while an (R^{2}) of (0) means the model explains none of the variance.\n",
        "- 2. Mean Absolute Error (MAE):\n",
        "  - Description: MAE is the average of the absolute differences between the predicted values and the actual values.\n",
        "  - Interpretation: A lower MAE indicates a better fit, as it represents the average magnitude of the errors. It is less sensitive to outliers compared to MSE/RMSE\n",
        "- 3. Root Mean Squared Error (RMSE):\n",
        "  - Description: RMSE is the square root of the average of the squared differences between predicted and actual values.\n",
        "  - Interpretation: A lower RMSE indicates better fit. By squaring the errors, it penalizes larger errors more heavily than MAE, making it more sensitive to outliers.     \n",
        "    \n",
        "**Question 8: What is the purpose of the R-squared metric in regression analysis?**\n",
        "\n",
        "Answer:\n",
        "- The purpose of the R-squared metric in regression analysis is to measure how well the independent variables in a model explain the variation in the dependent variable, indicating the goodness of fit. Also known as the coefficient of determination, it represents the proportion of the variance in the dependent variable that is predictable from the independent variables. An R-squared value of \\(100\\%\\) means the model explains all the variability, while \\(0\\%\\) means it explains none.\n",
        "- Measures goodness of fit: R-squared shows how closely the data points fit the regression line or curve.\n",
        "- Quantifies explanatory power: It quantifies the percentage of variance in the dependent variable that is accounted for by the independent variables in the model.\n",
        "- Ranges from 0 to 1 (or 0% to 100%): An R-squared of 0 indicates that the model does not explain any of the variance in the dependent variable.¬† An R-squared of 1 (or \\(100\\%\\)) indicates that the model perfectly explains all the variance.\n",
        "- Aids in model comparison: A higher R-squared value generally suggests a better fit, making it a useful metric for comparing different models for the same dataset.\n",
        "    \n",
        "     \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m6FrKZLFHnnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.\n",
        "\n",
        "#Answer:\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.random.rand(100, 1)  # Independent variable\n",
        "y = np.random.rand(100, 1)  # Dependent variable\n",
        "\n",
        "# Create a Linear Regression model object\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X,y)\n",
        "\n",
        "# Print the slope (coefficient) and intercept\n",
        "print(f\"Slope (m): {model.coef_[0]}\")\n",
        "print(f\"Intercept (c): {model.intercept_[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJBIFtx3Yrdp",
        "outputId": "75798f08-e574-4b5b-ab9d-d574f46639de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (m): [-0.00691144]\n",
            "Intercept (c): 0.5277974465288384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: How do you interpret the coefficients in a simple linear regression model?**\n",
        "\n",
        "Answer:\n",
        "- In a simple linear regression model (Y=a+bX), the coefficient (b) represents the average change in the dependent variable (Y) for a one-unit increase in the independent variable (X). The sign of the coefficient indicates the relationship's direction: a positive coefficient means (Y) increases as (X) increases, while a negative coefficient means (Y) decreases as (X) increases.\n",
        "- Intercept coefficient (a):\n",
        "  - Meaning: The intercept (a) is the predicted value of (Y) when (X) is zero. - Interpretation: The intercept is only a meaningful interpretation if it makes sense to have an (X) value of zero and if zero was included in your data range. If not, it primarily serves to anchor the regression line correctly and doesn't have a real-world interpretation.\n",
        "  - Example: In the equation (Y=10+2X), the intercept of (10) means that the model predicts (Y) to be (10) when (X) is (0)"
      ],
      "metadata": {
        "id": "1l1GeJ6DbJWF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uUpgm2GWZ6ng"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}